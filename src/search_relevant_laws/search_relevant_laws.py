import os
import json
import time
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
import requests

os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

# åŠ è½½PDFæ–‡ä»¶å¹¶æ„å»ºfaisså‘é‡ç´¢å¼•
def build_faiss_index(folder_path, index_name):
    """
    æ„å»ºFAISSå‘é‡ç´¢å¼•
    :param folder_path: PDFæ–‡ä»¶å¤¹è·¯å¾„
    :param index_name: ç´¢å¼•åç§°ï¼ˆç”¨äºåŒºåˆ†ä¸åŒç±»å‹çš„å†…å®¹ï¼‰
    :return: å‘é‡å­˜å‚¨å¯¹è±¡
    """
    # åŠ è½½PDF
    loader = DirectoryLoader(
        path=folder_path,
        glob="**/*.pdf",
        loader_cls=PyPDFLoader,
        use_multithreading=True
    )
    documents = loader.load()
    print(f"[{index_name}] å…±åŠ è½½ {len(documents)} é¡µPDFæ–‡æ¡£")
    
    # åˆ†å—å¤„ç†
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=512,
        chunk_overlap=50,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ"]  # æ ¹æ®ä¸­æ–‡æ ‡ç‚¹åˆ†éš”
    )
    texts = text_splitter.split_documents(documents)
    print(f"[{index_name}] ç”Ÿæˆ {len(texts)} ä¸ªæ–‡æœ¬å—")
    
    # åˆ›å»ºåµŒå…¥æ¨¡å‹
    embeddings = HuggingFaceEmbeddings(
        model_name="shibing624/text2vec-base-chinese",
        model_kwargs={'device': 'cpu'},
        cache_folder="./models/text2vec-base-chinese",
    )
    
    # æ„å»ºFAISSç´¢å¼•
    vector_store = FAISS.from_documents(texts, embeddings)
    print(f"[{index_name}] FAISSç´¢å¼•æ„å»ºå®Œæˆï¼ŒåŒ…å« {vector_store.index.ntotal} ä¸ªå‘é‡")
    
    return vector_store

# é€šè¿‡faissç´¢å¼•æ£€ç´¢æ¡ˆä»¶ä¿¡æ¯
def retrieve_case_context(vector_store, case_name, top_k=10):
    """
    æ£€ç´¢ç‰¹å®šæ¡ˆä»¶çš„ç›¸å…³å†…å®¹
    :param vector_store: æ¡ˆä»¶æ‘˜è¦å‘é‡åº“
    :param case_name: æ¡ˆä»¶åç§°
    :param top_k: è¿”å›ç»“æœæ•°é‡
    :return: æ¡ˆä»¶ç›¸å…³æ–‡æœ¬å†…å®¹
    """
    # ä½¿ç”¨æ¡ˆä»¶åç§°ä½œä¸ºæŸ¥è¯¢
    docs = vector_store.similarity_search(case_name, k=top_k)

    # è·å–æ–‡ä»¶åŸºæœ¬åï¼ˆä¸å«è·¯å¾„å’Œæ‰©å±•åï¼‰
    base_name = os.path.splitext(os.path.basename(case_name))[0]
    
    # è¿‡æ»¤å‡ºå±äºè¯¥æ¡ˆä»¶çš„æ–‡æœ¬å—
    case_context = []
    for doc in docs:
        # ä»å…ƒæ•°æ®è·å–æºæ–‡ä»¶å
        source_path = doc.metadata.get('source', '')
        source_name = os.path.splitext(os.path.basename(source_path))[0]
        
        # ä½¿ç”¨æ–‡ä»¶ååŸºæœ¬éƒ¨åˆ†åŒ¹é…
        if source_name == base_name:
            case_context.append(doc.page_content)
    
    return "\n\n".join(case_context)

# æ£€ç´¢ç›¸å…³æ³•å¾‹æ¡æ–‡
def retrieve_related_laws(law_vector_store, case_context, top_k=5):
    """
    æ£€ç´¢ä¸æ¡ˆä»¶ç›¸å…³çš„æ³•å¾‹æ¡æ–‡
    :param law_vector_store: æ³•å¾‹æ¡æ–‡å‘é‡åº“
    :param case_context: æ¡ˆä»¶ç›¸å…³å†…å®¹
    :param top_k: è¿”å›ç»“æœæ•°é‡
    :return: ç›¸å…³æ³•å¾‹æ¡æ–‡åˆ—è¡¨
    """
    # ä½¿ç”¨æ¡ˆä»¶å†…å®¹ä½œä¸ºæŸ¥è¯¢
    docs = law_vector_store.similarity_search(case_context, k=top_k)
    
    # æå–æ³•å¾‹æ¡æ–‡å†…å®¹
    laws = []
    for doc in docs:
        # è·å–æ³•å¾‹æ¡æ–‡æ¥æºä¿¡æ¯
        source_path = doc.metadata.get('source', '')
        law_name = os.path.splitext(os.path.basename(source_path))[0]
        
        # ç»„åˆæ³•å¾‹æ¡æ–‡ä¿¡æ¯
        law_info = {
            "æ¥æº": law_name,
            "å†…å®¹": doc.page_content
        }
        laws.append(law_info)
    
    return laws

# è°ƒç”¨DeepSeek APIç”Ÿæˆç»“æ„åŒ–JSON
def generate_case_json(context, related_laws, api_key):
    """
    ç”Ÿæˆæ¡ˆä»¶ç»“æ„åŒ–ä¿¡æ¯
    :param context: æ¡ˆä»¶ç›¸å…³å†…å®¹
    :param related_laws: ç›¸å…³æ³•å¾‹æ¡æ–‡
    :param api_key: DeepSeek APIå¯†é’¥
    :return: ç»“æ„åŒ–æ¡ˆä»¶ä¿¡æ¯
    """
    # æ ¼å¼åŒ–ç›¸å…³æ³•å¾‹æ¡æ–‡
    laws_text = "\n".join([f"{idx+1}. ã€Š{law['æ¥æº']}ã€‹: {law['å†…å®¹']}" 
                          for idx, law in enumerate(related_laws)])
    
    prompt = f"""
ä½ æ˜¯ä¸€åèµ„æ·±æ³•å¾‹ä¸“å®¶ï¼Œè¯·ä»ä»¥ä¸‹æ³•å¾‹æ–‡ä¹¦ä¸­æå–å…³é”®ä¿¡æ¯å¹¶è¾“å‡ºJSONæ ¼å¼ï¼š
1. æ¡ˆä»¶ç±»å‹ï¼ˆå¿…é¡»å±äºï¼šåŠ³åŠ¨çº çº·ã€ç¦»å©šã€æ°‘é—´å€Ÿè´·ä¸‰ç±»ä¹‹ä¸€ï¼‰
2. æ¡ˆç”±
3. å½“äº‹äººä¿¡æ¯ï¼ˆåŒ…æ‹¬åŸå‘Šå’Œè¢«å‘Šï¼‰
4. äº‰è®®ç„¦ç‚¹
5. è¯‰è®¼è¯·æ±‚
6. è£åˆ¤è¦ç‚¹
7. ç›¸å…³æ³•å¾‹æ¡æ–‡åŸæ–‡ï¼ˆåŸºäºæä¾›çš„æ¡æ–‡åˆ—è¡¨ï¼Œä¸å¯ä¿®æ”¹ï¼‰
8. å¦‚ä½•ä¾æ®æ³•å¾‹æ¡æ–‡å¯¹æ¡ˆä»¶åŠ ä»¥åˆ¤å†³ï¼ˆä¸ç›¸å…³æ³•å¾‹æ¡æ–‡åŸæ–‡ä¸€ä¸€å¯¹åº”ï¼‰
9. å…¶ä»–å…³é”®ä¿¡æ¯

### æ¡ˆä»¶æ–‡ä¹¦å†…å®¹ï¼š
{context}

### ç›¸å…³æ³•å¾‹æ¡æ–‡ï¼š
{laws_text}

### è¾“å‡ºè¦æ±‚ï¼š
- ä¸¥æ ¼ä½¿ç”¨JSONæ ¼å¼
- æ¡ˆä»¶ç±»å‹å¿…é¡»æ˜¯"åŠ³åŠ¨çº çº·"ã€"ç¦»å©š"æˆ–"æ°‘é—´å€Ÿè´·"
- å½“äº‹äººä¿¡æ¯æ ¼å¼ï¼š{{"åŸå‘Š": ["å§“å1", "å§“å2"], "è¢«å‘Š": ["å§“å1", "å§“å2"]}}
- ç›¸å…³æ³•å¾‹æ¡æ–‡åŠä¾æ®æ¡æ–‡çš„é€»è¾‘æ¨æ–­æ ¼å¼ï¼š[{{"æ¡çº¹åç§°"ï¼š"åç§°"ï¼Œ"æ¡æ–‡å†…å®¹": "å†…å®¹"ï¼Œ"é€»è¾‘æ¨æ–­"ï¼š"æ¨æ–­"}}]
- å…¶ä»–å­—æ®µä½¿ç”¨æ•°ç»„æ ¼å¼
- ç›¸å…³æ³•å¾‹æ¡æ–‡å°½å¯èƒ½å¤šï¼Œæ¯ä¸ªæ¡ˆä»¶è‡³å°‘å…­æ¡
- ç›¸å…³æ³•å¾‹æ¡æ–‡å°½å¯èƒ½æ¥è‡ªä¸åŒçš„æ³•å¾‹æ–‡ä»¶ï¼Œå¦‚â€œæ°‘æ³•å…¸â€ä¸â€œåŠ³åŠ¨åˆåŒæ³•â€æ˜¯ä¸¤ä¸ªä¸åŒçš„æ³•å¾‹æ–‡ä»¶
- ä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ€§æ–‡å­—
- ä¸è®¸å·æ‡’ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼
"""
    
    # è°ƒç”¨DeepSeek API
    url = "https://api.deepseek.com/chat/completions"
    headers = {"Authorization": f"Bearer {api_key}"}
    data = {
        "model": "deepseek-chat",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.3,
        "max_tokens": 3000  # å¢åŠ tokené™åˆ¶ä»¥å®¹çº³æ³•å¾‹æ¡æ–‡
    }
    
    try:
        response = requests.post(url, json=data, headers=headers, timeout=120)
        content = response.json()["choices"][0]["message"]["content"]
        
        # å»é™¤å¯èƒ½çš„Markdownæ ‡è®°
        if content.strip().startswith('```'):
            content = content.strip().lstrip('`json').lstrip('`').rstrip('`').strip()
            content = content.split('```')[-1] if '```' in content else content
        
        # è§£æJSON
        return json.loads(content)
    except Exception as e:
        print(f"APIè°ƒç”¨å‡ºé”™: {str(e)}")
        return {"error": str(e)}

# ä¸»å¤„ç†å‡½æ•°
def process_cases_with_laws(case_folder, law_folder, api_key, output_file="case_law_results.json"):
    """
    å¤„ç†æ¡ˆä»¶å¹¶å…³è”ç›¸å…³æ³•å¾‹æ¡æ–‡
    :param case_folder: æ¡ˆä»¶æ‘˜è¦æ–‡ä»¶å¤¹è·¯å¾„
    :param law_folder: æ³•å¾‹æ¡æ–‡æ–‡ä»¶å¤¹è·¯å¾„
    :param api_key: DeepSeek APIå¯†é’¥
    :param output_file: è¾“å‡ºæ–‡ä»¶å
    :return: å¤„ç†ç»“æœ
    """
    start_time = time.time()
    
    # æ„å»ºæ¡ˆä»¶æ‘˜è¦ç´¢å¼•
    print("\n" + "="*60)
    print("âš–ï¸ å¼€å§‹æ„å»ºæ¡ˆä»¶æ‘˜è¦ç´¢å¼•...")
    case_vector_store = build_faiss_index(case_folder, "æ¡ˆä»¶æ‘˜è¦")
    
    # æ„å»ºæ³•å¾‹æ¡æ–‡ç´¢å¼•
    print("\n" + "="*60)
    print("ğŸ“œ å¼€å§‹æ„å»ºæ³•å¾‹æ¡æ–‡ç´¢å¼•...")
    law_vector_store = build_faiss_index(law_folder, "æ³•å¾‹æ¡æ–‡")
    
    # è·å–æ‰€æœ‰æ¡ˆä»¶æ–‡ä»¶
    case_files = []
    for root, _, files in os.walk(case_folder):
        for file in files:
            if file.lower().endswith(".pdf"):
                case_files.append({
                    "path": os.path.join(root, file),
                    "name": os.path.splitext(file)[0]
                })
    
    print(f"\nğŸ” æ‰¾åˆ° {len(case_files)} ä¸ªæ¡ˆä»¶PDFæ–‡ä»¶")
    
    # å¤„ç†æ¯ä¸ªæ¡ˆä»¶
    results = {}
    for i, case in enumerate(case_files):
        case_name = case["name"]
        print(f"\n{'='*60}")
        print(f"å¤„ç†æ¡ˆä»¶ ({i+1}/{len(case_files)}): {case_name}")
        
        # æ£€ç´¢æ¡ˆä»¶ä¸Šä¸‹æ–‡
        context = retrieve_case_context(case_vector_store, case_name)
        
        if not context:
            print(f"âš ï¸ æœªæ‰¾åˆ° {case_name} çš„ç›¸å…³å†…å®¹")
            continue
            
        # æ£€ç´¢ç›¸å…³æ³•å¾‹æ¡æ–‡
        print("ğŸ” æ£€ç´¢ç›¸å…³æ³•å¾‹æ¡æ–‡...")
        related_laws = retrieve_related_laws(law_vector_store, context)
        print(f"âœ… æ‰¾åˆ° {len(related_laws)} æ¡ç›¸å…³æ³•å¾‹æ¡æ–‡")
        
        # ç”Ÿæˆç»“æ„åŒ–JSONï¼ˆåŒ…å«æ³•å¾‹æ¡æ–‡ï¼‰
        print("ğŸ§  ç”Ÿæˆç»“æ„åŒ–æ¡ˆä»¶ä¿¡æ¯...")
        case_info = generate_case_json(context, related_laws, api_key)
        
        # æ·»åŠ åˆ°ç»“æœ
        results[case_name] = case_info
        print(f"âœ… æ¡ˆä»¶å¤„ç†å®Œæˆ")
    
    # ä¿å­˜ç»“æœ
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # è®¡ç®—å¤„ç†æ—¶é—´
    processing_time = time.time() - start_time
    print(f"\n{'='*60}")
    print(f"âœ… æˆåŠŸå¤„ç† {len(results)} ä¸ªæ¡ˆä»¶")
    print(f"â±ï¸ æ€»è€—æ—¶: {processing_time:.2f}ç§’")
    print(f"ğŸ“„ ç»“æœå·²ä¿å­˜è‡³ {output_file}")
    
    return results

if __name__ == "__main__":
    # é…ç½®å‚æ•°
    CASE_FOLDER = r"C:\code\rag_framed\faiss_RAGextractor\data2"  # æ¡ˆä»¶æ‘˜è¦æ–‡ä»¶å¤¹
    LAW_FOLDER = r"C:\code\rag_framed\data1\laws"    # æ³•å¾‹æ¡æ–‡æ–‡ä»¶å¤¹
    API_KEY = "sk-c1d6cb5fc75c4de5ba19fa2b3f1143a1" 
    
    # æ‰§è¡Œå¤„ç†
    results = process_cases_with_laws(CASE_FOLDER, LAW_FOLDER, API_KEY)
    
    # æ‰“å°ç¤ºä¾‹è¾“å‡º
    if results:
        first_case = next(iter(results.values()))
        print("\n" + "="*60)
        print("ç¤ºä¾‹æ¡ˆä»¶è¾“å‡º:")
        print(json.dumps(first_case, ensure_ascii=False, indent=2))